{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86570ab5-9f88-4d5c-9e1e-12d4dd586fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "import csv\n",
    "df = pd.read_csv(\"C:/Users/melek/Downloads/large_student_answer_dataset.csv\")\n",
    "\n",
    "# Cümlelere ayırma\n",
    "# Metin örneğimiz\n",
    "text = \"Gravity keeps us on the ground. It affects all objects on Earth.\"\n",
    "\n",
    "# Cümlelere böl\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Lemmatizer ve Stemmer'ı başlat\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Stopwords listesini almak\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Kelimeleri tokenleştirip, lemmatize etme ve stemleme\n",
    "def preprocess_sentence(sentence):\n",
    "    tokens = word_tokenize(sentence)  # Cümleyi kelimelere ayır\n",
    "    # Sadece harf olan kelimeleri al ve stopword'leri çıkar\n",
    "    filtered_tokens = [token.lower() for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
    "    \n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]  # Lemmatize etme\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]  # Stemleme\n",
    "    \n",
    "    return lemmatized_tokens, stemmed_tokens\n",
    "\n",
    "# Her cümleyi tokenleştir, lemmatize et ve stemle\n",
    "tokenized_corpus_lemmatized = []\n",
    "tokenized_corpus_stemmed = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    lemmatized_tokens, stemmed_tokens = preprocess_sentence(sentence)\n",
    "    tokenized_corpus_lemmatized.append(lemmatized_tokens)\n",
    "    tokenized_corpus_stemmed.append(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d21014fe-ea93-4925-92bc-661495ba79ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['gravity', 'keep', 'u', 'ground'], ['affect', 'object', 'earth']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_corpus_lemmatized[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "231d7c85-af9e-4af7-b69b-83ff06af19d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gravity keep u ground', 'affect object earth']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Ön işlenmiş token listelerini tekrar metne çeviriyoruz\n",
    "lemmatized_texts = [' '.join(tokens) for tokens in tokenized_corpus_lemmatized]\n",
    "\n",
    "lemmatized_texts[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ec98066-e5ce-4acd-8f8b-93e705954c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    affect    earth  gravity   ground     keep   object\n",
      "0  0.00000  0.00000  0.57735  0.57735  0.57735  0.00000\n",
      "1  0.57735  0.57735  0.00000  0.00000  0.00000  0.57735\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF vektörizerı başlatıyoruz\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# TF-IDF matrisini oluşturuyoruz\n",
    "#terim frekansları, belge frekanslarıni hesplar\n",
    "#TF-IDF vektörlerine dönüştürür\n",
    "tfidf_matrix = vectorizer.fit_transform(lemmatized_texts)\n",
    "\n",
    "## Kelimeleri alalım\n",
    "#F-IDF vektörleştirme işleminde kullanılan tüm kelimelerin essiz bir listesini döndürur\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# TF-IDF matrisini pandas DataFrame'e çevir-gorunurluk acisindan- calismasi kolay\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "# İlk birkaç satırı gösterelim-ilk 5 cümle\n",
    "print(tfidf_df.head())\n",
    "\n",
    "#Her satır bir cümleyi temsil eder\n",
    "#Her sütun bir kelimeyi temsil eder\n",
    "#Hücreler ise o kelimenin o cümledeki TF-IDF skorudur - her cumle icin degisir-bakiniz:slaytlar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68ec56cd-5385-4193-bccf-97e00dca83f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "İlk cümlede en yüksek TF-IDF skoruna sahip 5 kelime:\n",
      "gravity    0.57735\n",
      "ground     0.57735\n",
      "keep       0.57735\n",
      "affect     0.00000\n",
      "earth      0.00000\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# İlk cümle için TF-IDF skorlarını al\n",
    "first_sentence_vector = tfidf_df.iloc[0]\n",
    "\n",
    "# Skorlara göre sırala (yüksekten düşüğe)\n",
    "top_5_words = first_sentence_vector.sort_values(ascending=False).head(5)\n",
    "\n",
    "# Sonucu yazdır\n",
    "print(\"İlk cümlede en yüksek TF-IDF skoruna sahip 5 kelime:\")\n",
    "print(top_5_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab054d55-1cca-493a-b5fd-68f1ec9a0205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object: 1.0000\n",
      "earth: 1.0000\n",
      "affect: 1.0000\n",
      "keep: 0.0000\n",
      "ground: 0.0000\n",
      "gravity: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Python kelimesinin vektörünü alalım\n",
    "affect_index = feature_names.tolist().index('affect')  # 'python' kelimesinin indeksini bul\n",
    "\n",
    "# Python kelimesinin TF-IDF vektörünü alıyoruz ve 2D formatta yapıyoruz\n",
    "affect_vector = tfidf_matrix[:, affect_index].toarray()\n",
    "\n",
    "# Tüm kelimelerin TF-IDF vektörlerini alıyoruz\n",
    "tfidf_vectors = tfidf_matrix.toarray()\n",
    "\n",
    "# Cosine similarity hesaplayalım\n",
    "similarities = cosine_similarity(affect_vector.T, tfidf_vectors.T)\n",
    "\n",
    "# Benzerlikleri sıralayalım ve en yüksek 5 kelimeyi seçelim\n",
    "similarities = similarities.flatten()\n",
    "top_5_indices = similarities.argsort()[-6:][::-1]  # 6. en büyükten başlıyoruz çünkü kendisi de dahil\n",
    "\n",
    "# Sonuçları yazdıralım\n",
    "for index in top_5_indices:\n",
    "    print(f\"{feature_names[index]}: {similarities[index]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bfab8d-cf7f-4458-953d-145f29618222",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d94050e-6ccf-4b0c-a73d-f6688b040819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aee989-8b62-450c-add3-8f8fee2a6c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
